# Just-Dist

A minimal, thread-aware C++ project simulating and benchmarking industry-grade **all-reduce** algorithms and **parallel training strategies** like **data parallelism** and **pipeline parallelism**, using `libtorch` tensors for realism.

---

## ğŸ’¡ Project Vision

Simulate how distributed systems perform **tensor communication** and **model parallel training**, like real-world LLM and DNN frameworks (e.g. Megatron, DeepSpeed, PipeDream, etc.).  
The goal is **not** to do actual distributed compute, but to model and analyze it realistically using threads, message-passing, and CPU-based tensor operations.

---

## ğŸ§± Architecture Overview

```
[ Node0 ] â”€â”€â”€â”€â”€â”
[ Node1 ] â”€â”€â”€â” â”‚   --> Simulated topology (ring/tree/etc.)
[ Node2 ] â”€â” â”‚ â”‚
    ...    â†“ â†“ â†“
[ CommBus / AllReduceAlgo ] <--- implements algorithm over N nodes
[ Scheduler ] <-- executes simulated forward/backward passes
[ Timer/Logger ] <-- logs performance per iteration
```

---

## âš™ï¸ System Requirements

- Ubuntu 20.04+
- `libtorch` (C++ PyTorch) â€” CPU as a start , GPU next
- `g++` 9+ (C++17 or later)
- `CMake` 3.18+

---

## ğŸ—ï¸ Project Setup

1. **Clone the repo**

```bash
git clone https://github.com/yourname/Just-Dist.git
cd Just-Dist
```

2. **Download libtorch** CPU version:
   https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-2.7.1.zip
   Unzip to `Just-Dist/libtorch/`.

3. **Build the project**

```bash
mkdir build && cd build
cmake ..
make -j$(nproc)
./just_dist
```

---

## ğŸ§© Implementation Order

### ğŸ“ Phase 1: Core Infrastructure
* âœ… `main.cpp` for bootstrapping
* âœ… Setup `CMakeLists.txt`
* âœ… Define `Node` struct/class (represents one device)
* âœ… Implement simple `RingAllReduce` with threads + tensor addition
* âœ… Write benchmarking wrapper using `std::chrono`

### ğŸ“ Phase 2: All-Reduce Algorithms
* Ring All-Reduce (`comm/ring_allreduce.cpp`)
* Tree All-Reduce
* Halving-Doubling
* Butterfly
* Compare latency vs #nodes, tensor size

### ğŸ“ Phase 3: Scheduler & Pipeline Sim
* Implement forward-backward pipeline (e.g. 4-layer toy model)
* Each layer = thread on a node with delay
* Use condition variables or semaphores to simulate stage dependencies

### ğŸ“ Phase 4: Benchmark & Logging
* Write JSON/CSV logs: `{algo, time, num_nodes, tensor_shape}`
* Visualize later with matplotlib or JS timeline

### ğŸ“ Phase 5 (Optional): Fancy
* Simulate bandwidth caps & latency
* Use profiler macros like `PROFILE_SCOPE("ring_reduce")`
* Plug in real GPU + CUDA later if needed

---

## ğŸ“¦ Folder Structure

```
Just-Dist/
â”œâ”€â”€ CMakeLists.txt
â”œâ”€â”€ include/
â”‚   â””â”€â”€ just_dist/             # public headers
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.cpp
â”‚   â”œâ”€â”€ comm/                  # allreduce algorithms
â”‚   â””â”€â”€ scheduler/             # pipeline/data-parallel scheduling
â”œâ”€â”€ libtorch/                  # external lib (not committed)
â”œâ”€â”€ build/                     # cmake build dir
â””â”€â”€ README.md
```

---

## ğŸ“œ License

MIT â€” use, fork, learn, modify freely.

## ğŸ™‹â€â™‚ï¸ Author

Ghassen Fatnassi â€” cracking distributed AI one thread at a time.